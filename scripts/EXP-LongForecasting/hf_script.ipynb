{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Traffic prediction_length is 24. Reference:\n",
    "# https://github.com/awslabs/gluonts/blob/6605ab1278b6bf92d5e47343efcf0d22bc50b2ec/src/gluonts/dataset/repository/_lstnet.py#L105\n",
    "\n",
    "prediction_length = 24\n",
    "context_length = prediction_length*2\n",
    "batch_size = 128\n",
    "num_batches_per_epoch = 100\n",
    "epochs = 50\n",
    "scaling = \"std\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_layers=2\n",
    "decoder_layers=2\n",
    "d_model=16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gluonts.dataset.repository.datasets import get_dataset\n",
    "\n",
    "dataset = get_dataset(\"traffic\")\n",
    "freq = dataset.metadata.freq\n",
    "prediction_length = dataset.metadata.prediction_length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'H'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_example = next(iter(dataset.train))\n",
    "test_example = next(iter(dataset.test))\n",
    "\n",
    "num_of_samples = 4*prediction_length\n",
    "\n",
    "figure, axes = plt.subplots()\n",
    "axes.plot(train_example[\"target\"][-num_of_samples:], color=\"blue\")\n",
    "axes.plot(\n",
    "    test_example[\"target\"][-num_of_samples - prediction_length :],\n",
    "    color=\"red\",\n",
    "    alpha=0.5,\n",
    ")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dataset.train\n",
    "test_dataset = dataset.test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('feat_dynamic_age', 'observed_values', 'time_feat', 'target')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FieldName.FEAT_AGE, FieldName.OBSERVED_VALUES,FieldName.FEAT_TIME, FieldName.TARGET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PretrainedConfig\n",
    "from gluonts.time_feature import time_features_from_frequency_str\n",
    "\n",
    "from gluonts.dataset.field_names import FieldName\n",
    "from gluonts.transform import (\n",
    "    AddAgeFeature,\n",
    "    AddObservedValuesIndicator,\n",
    "    AddTimeFeatures,\n",
    "    AsNumpyArray,\n",
    "    Chain,\n",
    "    ExpectedNumInstanceSampler,\n",
    "    RemoveFields,\n",
    "    SelectFields,\n",
    "    SetField,\n",
    "    TestSplitSampler,\n",
    "    Transformation,\n",
    "    ValidationSplitSampler,\n",
    "    VstackFeatures,\n",
    "    RenameFields,\n",
    ")\n",
    "\n",
    "def create_transformation(freq: str, config: PretrainedConfig) -> Transformation:\n",
    "    # create a list of fields to remove later\n",
    "    remove_field_names = []\n",
    "    if config.num_static_real_features == 0:\n",
    "        remove_field_names.append(FieldName.FEAT_STATIC_REAL)\n",
    "    if config.num_dynamic_real_features == 0:\n",
    "        remove_field_names.append(FieldName.FEAT_DYNAMIC_REAL)\n",
    "    if config.num_static_categorical_features == 0:\n",
    "        remove_field_names.append(FieldName.FEAT_STATIC_CAT)\n",
    "\n",
    "    return Chain(\n",
    "        # step 1: remove static/dynamic fields if not specified\n",
    "        [RemoveFields(field_names=remove_field_names)]\n",
    "        # step 2: convert the data to NumPy (potentially not needed)\n",
    "        + (\n",
    "            [\n",
    "                AsNumpyArray(\n",
    "                    field=FieldName.FEAT_STATIC_CAT,\n",
    "                    expected_ndim=1,\n",
    "                    dtype=int,\n",
    "                )\n",
    "            ]\n",
    "            if config.num_static_categorical_features > 0\n",
    "            else []\n",
    "        )\n",
    "        + (\n",
    "            [\n",
    "                AsNumpyArray(\n",
    "                    field=FieldName.FEAT_STATIC_REAL,\n",
    "                    expected_ndim=1,\n",
    "                )\n",
    "            ]\n",
    "            if config.num_static_real_features > 0\n",
    "            else []\n",
    "        )\n",
    "        + [\n",
    "            AsNumpyArray(\n",
    "                field=FieldName.TARGET,\n",
    "                # we expect an extra dim for the multivariate case:\n",
    "                expected_ndim=1 if config.input_size == 1 else 2,\n",
    "            ),\n",
    "            # step 3: handle the NaN's by filling in the target with zero\n",
    "            # and return the mask (which is in the observed values)\n",
    "            # true for observed values, false for nan's\n",
    "            # the decoder uses this mask (no loss is incurred for unobserved values)\n",
    "            # see loss_weights inside the xxxForPrediction model\n",
    "            AddObservedValuesIndicator(\n",
    "                target_field=FieldName.TARGET,\n",
    "                output_field=FieldName.OBSERVED_VALUES,\n",
    "            ),\n",
    "            # step 4: add temporal features based on freq of the dataset\n",
    "            # these serve as positional encodings\n",
    "            AddTimeFeatures(\n",
    "                start_field=FieldName.START,\n",
    "                target_field=FieldName.TARGET,\n",
    "                output_field=FieldName.FEAT_TIME,\n",
    "                time_features=time_features_from_frequency_str(freq),\n",
    "                pred_length=config.prediction_length,\n",
    "            ),\n",
    "            # step 5: add another temporal feature (just a single number)\n",
    "            # tells the model where in the life the value of the time series is\n",
    "            # sort of running counter\n",
    "            AddAgeFeature(\n",
    "                target_field=FieldName.TARGET,\n",
    "                output_field=FieldName.FEAT_AGE,\n",
    "                pred_length=config.prediction_length,\n",
    "                log_scale=True,\n",
    "            ),\n",
    "            # step 6: vertically stack all the temporal features into the key FEAT_TIME\n",
    "            VstackFeatures(\n",
    "                output_field=FieldName.FEAT_TIME,\n",
    "                input_fields=[FieldName.FEAT_TIME, FieldName.FEAT_AGE]\n",
    "                + (\n",
    "                    [FieldName.FEAT_DYNAMIC_REAL]\n",
    "                    if config.num_dynamic_real_features > 0\n",
    "                    else []\n",
    "                ),\n",
    "            ),\n",
    "            # step 7: rename to match HuggingFace names\n",
    "            RenameFields(\n",
    "                mapping={\n",
    "                    FieldName.FEAT_STATIC_CAT: \"static_categorical_features\",\n",
    "                    FieldName.FEAT_STATIC_REAL: \"static_real_features\",\n",
    "                    FieldName.FEAT_TIME: \"time_features\",\n",
    "                    FieldName.TARGET: \"values\",\n",
    "                    FieldName.OBSERVED_VALUES: \"observed_mask\",\n",
    "                }\n",
    "            ),\n",
    "        ]\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gluonts.transform import InstanceSplitter\n",
    "from gluonts.transform.sampler import InstanceSampler\n",
    "from typing import Optional\n",
    "\n",
    "\n",
    "def create_instance_splitter(\n",
    "    config: PretrainedConfig,\n",
    "    mode: str,\n",
    "    train_sampler: Optional[InstanceSampler] = None,\n",
    "    validation_sampler: Optional[InstanceSampler] = None,\n",
    ") -> Transformation:\n",
    "    assert mode in [\"train\", \"validation\", \"test\"]\n",
    "\n",
    "    instance_sampler = {\n",
    "        \"train\": train_sampler\n",
    "        or ExpectedNumInstanceSampler(\n",
    "            num_instances=1.0, min_future=config.prediction_length\n",
    "        ),\n",
    "        \"validation\": validation_sampler\n",
    "        or ValidationSplitSampler(min_future=config.prediction_length),\n",
    "        \"test\": TestSplitSampler(),\n",
    "    }[mode]\n",
    "\n",
    "    return InstanceSplitter(\n",
    "        target_field=\"values\",\n",
    "        is_pad_field=FieldName.IS_PAD,\n",
    "        start_field=FieldName.START,\n",
    "        forecast_start_field=FieldName.FORECAST_START,\n",
    "        instance_sampler=instance_sampler,\n",
    "        past_length=config.context_length + max(config.lags_sequence),\n",
    "        future_length=config.prediction_length,\n",
    "        time_series_fields=[\"time_features\", \"observed_mask\"],\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterable\n",
    "\n",
    "import torch\n",
    "from gluonts.itertools import Cyclic, Cached\n",
    "from gluonts.dataset.loader import as_stacked_batches\n",
    "\n",
    "\n",
    "def create_train_dataloader(\n",
    "    config: PretrainedConfig,\n",
    "    freq,\n",
    "    data,\n",
    "    batch_size: int,\n",
    "    num_batches_per_epoch: int,\n",
    "    shuffle_buffer_length: Optional[int] = None,\n",
    "    cache_data: bool = True,\n",
    "    **kwargs,\n",
    ") -> Iterable:\n",
    "    PREDICTION_INPUT_NAMES = [\n",
    "        \"past_time_features\",\n",
    "        \"past_values\",\n",
    "        \"past_observed_mask\",\n",
    "        \"future_time_features\",\n",
    "    ]\n",
    "    if config.num_static_categorical_features > 0:\n",
    "        PREDICTION_INPUT_NAMES.append(\"static_categorical_features\")\n",
    "\n",
    "    if config.num_static_real_features > 0:\n",
    "        PREDICTION_INPUT_NAMES.append(\"static_real_features\")\n",
    "\n",
    "    TRAINING_INPUT_NAMES = PREDICTION_INPUT_NAMES + [\n",
    "        \"future_values\",\n",
    "        \"future_observed_mask\",\n",
    "    ]\n",
    "\n",
    "    transformation = create_transformation(freq, config)\n",
    "    transformed_data = transformation.apply(data, is_train=True)\n",
    "    if cache_data:\n",
    "        transformed_data = Cached(transformed_data)\n",
    "\n",
    "    # we initialize a Training instance\n",
    "    instance_splitter = create_instance_splitter(config, \"train\")\n",
    "\n",
    "    # the instance splitter will sample a window of\n",
    "    # context length + lags + prediction length (from the 366 possible transformed time series)\n",
    "    # randomly from within the target time series and return an iterator.\n",
    "    stream = Cyclic(transformed_data).stream()\n",
    "    training_instances = instance_splitter.apply(stream, is_train=True)\n",
    "\n",
    "    return as_stacked_batches(\n",
    "        training_instances,\n",
    "        batch_size=batch_size,\n",
    "        shuffle_buffer_length=shuffle_buffer_length,\n",
    "        field_names=TRAINING_INPUT_NAMES,\n",
    "        output_type=torch.tensor,\n",
    "        num_batches_per_epoch=num_batches_per_epoch,\n",
    "    )\n",
    "\n",
    "def create_test_dataloader(\n",
    "    config: PretrainedConfig,\n",
    "    freq,\n",
    "    data,\n",
    "    batch_size: int,\n",
    "    **kwargs,\n",
    "):\n",
    "    PREDICTION_INPUT_NAMES = [\n",
    "        \"past_time_features\",\n",
    "        \"past_values\",\n",
    "        \"past_observed_mask\",\n",
    "        \"future_time_features\",\n",
    "    ]\n",
    "    if config.num_static_categorical_features > 0:\n",
    "        PREDICTION_INPUT_NAMES.append(\"static_categorical_features\")\n",
    "\n",
    "    if config.num_static_real_features > 0:\n",
    "        PREDICTION_INPUT_NAMES.append(\"static_real_features\")\n",
    "\n",
    "    transformation = create_transformation(freq, config)\n",
    "    transformed_data = transformation.apply(data, is_train=False)\n",
    "\n",
    "    # we create a Test Instance splitter which will sample the very last\n",
    "    # context window seen during training only for the encoder.\n",
    "    instance_sampler = create_instance_splitter(config, \"test\")\n",
    "\n",
    "    # we apply the transformations in test mode\n",
    "    testing_instances = instance_sampler.apply(transformed_data, is_train=False)\n",
    "\n",
    "    return as_stacked_batches(\n",
    "        testing_instances,\n",
    "        batch_size=batch_size,\n",
    "        output_type=torch.tensor,\n",
    "        field_names=PREDICTION_INPUT_NAMES,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoformerConfig, AutoformerForPrediction\n",
    "\n",
    "config = AutoformerConfig.from_pretrained(\"kashif/autoformer-traffic-hourly\")\n",
    "model = AutoformerForPrediction.from_pretrained(\"kashif/autoformer-traffic-hourly\")\n",
    "\n",
    "test_dataloader = create_test_dataloader(\n",
    "    config=config,\n",
    "    freq=freq,\n",
    "    data=test_dataset,\n",
    "    batch_size=64,\n",
    ")\n",
    "\n",
    "train_dataloader = create_test_dataloader(\n",
    "    config=config,\n",
    "    freq=freq,\n",
    "    data=train_dataset,\n",
    "    batch_size=1,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'target': array([0.0048, 0.0072, 0.004 , ..., 0.053 , 0.0533, 0.05  ], dtype=float32),\n",
       " 'start': Period('2015-01-01 00:00', 'H'),\n",
       " 'feat_static_cat': array([0], dtype=int32),\n",
       " 'item_id': 0}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = next(iter(train_dataset))\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 769, 5]),\n",
       " torch.Size([1, 769]),\n",
       " torch.Size([1, 769]),\n",
       " torch.Size([1, 24, 5]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# batch = next(iter(train_dataloader))\n",
    "batch['past_time_features'].shape, batch['past_values'].shape, batch['past_observed_mask'].shape, batch['future_time_features'].shape, \n",
    "# batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:accelerate.utils.other:Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "2023-11-23 20:18:46.128437: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from accelerate import Accelerator\n",
    "\n",
    "accelerator = Accelerator()\n",
    "device = accelerator.device\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "forecasts_ = []\n",
    "for batch in test_dataloader:\n",
    "    outputs = model.generate(\n",
    "        static_categorical_features=batch[\"static_categorical_features\"].to(device)\n",
    "        if config.num_static_categorical_features > 0\n",
    "        else None,\n",
    "        static_real_features=batch[\"static_real_features\"].to(device)\n",
    "        if config.num_static_real_features > 0\n",
    "        else None,\n",
    "        past_time_features=batch[\"past_time_features\"].to(device),\n",
    "        past_values=batch[\"past_values\"].to(device),\n",
    "        future_time_features=batch[\"future_time_features\"].to(device),\n",
    "        past_observed_mask=batch[\"past_observed_mask\"].to(device),\n",
    "    )\n",
    "    forecasts_.append(outputs.sequences.cpu().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 100, 24)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecasts_[0].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6034, 100, 24)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "forecasts = np.vstack(forecasts_)\n",
    "print(forecasts.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.014170408248901367,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 6034,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "241d2e556bf3453ab746cc362ee57fb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6034 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.autonotebook import tqdm\n",
    "from evaluate import load\n",
    "from gluonts.time_feature import get_seasonality\n",
    "\n",
    "mase_metric = load(\"evaluate-metric/mase\")\n",
    "\n",
    "forecast_median = np.median(forecasts, 1)\n",
    "\n",
    "mase_metrics = []\n",
    "for item_id, ts in enumerate(tqdm(test_dataset)):\n",
    "    training_data = ts[\"target\"][:-prediction_length]\n",
    "    ground_truth = ts[\"target\"][-prediction_length:]\n",
    "    mase = mase_metric.compute(\n",
    "        predictions=forecast_median[item_id], \n",
    "        references=np.array(ground_truth), \n",
    "        training=np.array(training_data), \n",
    "        periodicity=get_seasonality(freq))\n",
    "    mase_metrics.append(mase[\"mase\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autoformer univariate MASE: 0.911\n"
     ]
    }
   ],
   "source": [
    "print(f\"Autoformer univariate MASE: {np.mean(mase_metrics):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.dates as mdates\n",
    "import pandas as pd\n",
    "\n",
    "test_ds = list(test_dataset)\n",
    "\n",
    "def plot(ts_index):\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    index = pd.period_range(\n",
    "        start=test_ds[ts_index][FieldName.START],\n",
    "        periods=len(test_ds[ts_index][FieldName.TARGET]),\n",
    "        freq=test_ds[ts_index][FieldName.START].freq,\n",
    "    ).to_timestamp()\n",
    "\n",
    "    ax.plot(\n",
    "        index[-5*prediction_length:], \n",
    "        test_ds[ts_index][\"target\"][-5*prediction_length:],\n",
    "        label=\"actual\",\n",
    "    )\n",
    "\n",
    "    plt.plot(\n",
    "        index[-prediction_length:], \n",
    "        np.median(forecasts[ts_index], axis=0),\n",
    "        label=\"median\",\n",
    "    )\n",
    "    \n",
    "    plt.gcf().autofmt_xdate()\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot(4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gluonts.torch.model.d_linear.estimator import DLinearEstimator\n",
    "\n",
    "# Define the DLinear model with the same parameters as the Autoformer model\n",
    "estimator = DLinearEstimator(\n",
    "    prediction_length=dataset.metadata.prediction_length,\n",
    "    context_length=dataset.metadata.prediction_length*2,\n",
    "    scaling=scaling,\n",
    "    hidden_dimension=2,\n",
    "    \n",
    "    batch_size=batch_size,\n",
    "    num_batches_per_epoch=num_batches_per_epoch,\n",
    "    trainer_kwargs=dict(max_epochs=epochs)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Trainer will use only 1 of 8 GPUs because it is running inside an interactive / notebook environment. You may try to set `Trainer(devices=8)` but please note that multi-GPU inside interactive / notebook environments is considered experimental and unstable. Your mileage may vary.\n",
      "INFO:lightning.pytorch.utilities.rank_zero:Trainer will use only 1 of 8 GPUs because it is running inside an interactive / notebook environment. You may try to set `Trainer(devices=8)` but please note that multi-GPU inside interactive / notebook environments is considered experimental and unstable. Your mileage may vary.\n",
      "INFO: GPU available: True (cuda), used: True\n",
      "INFO:lightning.pytorch.utilities.rank_zero:GPU available: True (cuda), used: True\n",
      "INFO: TPU available: False, using: 0 TPU cores\n",
      "INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO: IPU available: False, using: 0 IPUs\n",
      "INFO:lightning.pytorch.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
      "INFO: HPU available: False, using: 0 HPUs\n",
      "INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
      "/home/aw82/miniconda3/lib/python3.9/site-packages/lightning/pytorch/trainer/configuration_validator.py:74: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "INFO:lightning.pytorch.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "INFO: \n",
      "  | Name  | Type         | Params\n",
      "---------------------------------------\n",
      "0 | model | DLinearModel | 4.7 K \n",
      "---------------------------------------\n",
      "4.7 K     Trainable params\n",
      "0         Non-trainable params\n",
      "4.7 K     Total params\n",
      "0.019     Total estimated model params size (MB)\n",
      "INFO:lightning.pytorch.callbacks.model_summary:\n",
      "  | Name  | Type         | Params\n",
      "---------------------------------------\n",
      "0 | model | DLinearModel | 4.7 K \n",
      "---------------------------------------\n",
      "4.7 K     Trainable params\n",
      "0         Non-trainable params\n",
      "4.7 K     Total params\n",
      "0.019     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": "{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_noinv_fmt}{postfix}]",
       "colour": null,
       "elapsed": 0.015143632888793945,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Training",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b761a49d43941e69d603a27fe700bae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Epoch 0, global step 100: 'train_loss' reached -2.11967 (best -2.11967), saving model to '/home/aw82/StatML/enhancedts-transformers/scripts/EXP-LongForecasting/lightning_logs/version_3/checkpoints/epoch=0-step=100.ckpt' as top 1\n",
      "INFO:lightning.pytorch.utilities.rank_zero:Epoch 0, global step 100: 'train_loss' reached -2.11967 (best -2.11967), saving model to '/home/aw82/StatML/enhancedts-transformers/scripts/EXP-LongForecasting/lightning_logs/version_3/checkpoints/epoch=0-step=100.ckpt' as top 1\n",
      "INFO: Epoch 1, global step 200: 'train_loss' reached -2.41386 (best -2.41386), saving model to '/home/aw82/StatML/enhancedts-transformers/scripts/EXP-LongForecasting/lightning_logs/version_3/checkpoints/epoch=1-step=200.ckpt' as top 1\n",
      "INFO:lightning.pytorch.utilities.rank_zero:Epoch 1, global step 200: 'train_loss' reached -2.41386 (best -2.41386), saving model to '/home/aw82/StatML/enhancedts-transformers/scripts/EXP-LongForecasting/lightning_logs/version_3/checkpoints/epoch=1-step=200.ckpt' as top 1\n",
      "INFO: Epoch 2, global step 300: 'train_loss' reached -2.64936 (best -2.64936), saving model to '/home/aw82/StatML/enhancedts-transformers/scripts/EXP-LongForecasting/lightning_logs/version_3/checkpoints/epoch=2-step=300.ckpt' as top 1\n",
      "INFO:lightning.pytorch.utilities.rank_zero:Epoch 2, global step 300: 'train_loss' reached -2.64936 (best -2.64936), saving model to '/home/aw82/StatML/enhancedts-transformers/scripts/EXP-LongForecasting/lightning_logs/version_3/checkpoints/epoch=2-step=300.ckpt' as top 1\n",
      "INFO: Epoch 3, global step 400: 'train_loss' reached -2.82447 (best -2.82447), saving model to '/home/aw82/StatML/enhancedts-transformers/scripts/EXP-LongForecasting/lightning_logs/version_3/checkpoints/epoch=3-step=400.ckpt' as top 1\n",
      "INFO:lightning.pytorch.utilities.rank_zero:Epoch 3, global step 400: 'train_loss' reached -2.82447 (best -2.82447), saving model to '/home/aw82/StatML/enhancedts-transformers/scripts/EXP-LongForecasting/lightning_logs/version_3/checkpoints/epoch=3-step=400.ckpt' as top 1\n",
      "INFO: Epoch 4, global step 500: 'train_loss' reached -2.87645 (best -2.87645), saving model to '/home/aw82/StatML/enhancedts-transformers/scripts/EXP-LongForecasting/lightning_logs/version_3/checkpoints/epoch=4-step=500.ckpt' as top 1\n",
      "INFO:lightning.pytorch.utilities.rank_zero:Epoch 4, global step 500: 'train_loss' reached -2.87645 (best -2.87645), saving model to '/home/aw82/StatML/enhancedts-transformers/scripts/EXP-LongForecasting/lightning_logs/version_3/checkpoints/epoch=4-step=500.ckpt' as top 1\n",
      "INFO: Epoch 5, global step 600: 'train_loss' reached -2.89015 (best -2.89015), saving model to '/home/aw82/StatML/enhancedts-transformers/scripts/EXP-LongForecasting/lightning_logs/version_3/checkpoints/epoch=5-step=600.ckpt' as top 1\n",
      "INFO:lightning.pytorch.utilities.rank_zero:Epoch 5, global step 600: 'train_loss' reached -2.89015 (best -2.89015), saving model to '/home/aw82/StatML/enhancedts-transformers/scripts/EXP-LongForecasting/lightning_logs/version_3/checkpoints/epoch=5-step=600.ckpt' as top 1\n",
      "INFO: Epoch 6, global step 700: 'train_loss' reached -2.90869 (best -2.90869), saving model to '/home/aw82/StatML/enhancedts-transformers/scripts/EXP-LongForecasting/lightning_logs/version_3/checkpoints/epoch=6-step=700.ckpt' as top 1\n",
      "INFO:lightning.pytorch.utilities.rank_zero:Epoch 6, global step 700: 'train_loss' reached -2.90869 (best -2.90869), saving model to '/home/aw82/StatML/enhancedts-transformers/scripts/EXP-LongForecasting/lightning_logs/version_3/checkpoints/epoch=6-step=700.ckpt' as top 1\n",
      "INFO: Epoch 7, global step 800: 'train_loss' reached -2.91506 (best -2.91506), saving model to '/home/aw82/StatML/enhancedts-transformers/scripts/EXP-LongForecasting/lightning_logs/version_3/checkpoints/epoch=7-step=800.ckpt' as top 1\n",
      "INFO:lightning.pytorch.utilities.rank_zero:Epoch 7, global step 800: 'train_loss' reached -2.91506 (best -2.91506), saving model to '/home/aw82/StatML/enhancedts-transformers/scripts/EXP-LongForecasting/lightning_logs/version_3/checkpoints/epoch=7-step=800.ckpt' as top 1\n",
      "INFO: Epoch 8, global step 900: 'train_loss' reached -2.93141 (best -2.93141), saving model to '/home/aw82/StatML/enhancedts-transformers/scripts/EXP-LongForecasting/lightning_logs/version_3/checkpoints/epoch=8-step=900.ckpt' as top 1\n",
      "INFO:lightning.pytorch.utilities.rank_zero:Epoch 8, global step 900: 'train_loss' reached -2.93141 (best -2.93141), saving model to '/home/aw82/StatML/enhancedts-transformers/scripts/EXP-LongForecasting/lightning_logs/version_3/checkpoints/epoch=8-step=900.ckpt' as top 1\n",
      "INFO: Epoch 9, global step 1000: 'train_loss' was not in top 1\n",
      "INFO:lightning.pytorch.utilities.rank_zero:Epoch 9, global step 1000: 'train_loss' was not in top 1\n",
      "INFO: Epoch 10, global step 1100: 'train_loss' reached -2.93262 (best -2.93262), saving model to '/home/aw82/StatML/enhancedts-transformers/scripts/EXP-LongForecasting/lightning_logs/version_3/checkpoints/epoch=10-step=1100.ckpt' as top 1\n",
      "INFO:lightning.pytorch.utilities.rank_zero:Epoch 10, global step 1100: 'train_loss' reached -2.93262 (best -2.93262), saving model to '/home/aw82/StatML/enhancedts-transformers/scripts/EXP-LongForecasting/lightning_logs/version_3/checkpoints/epoch=10-step=1100.ckpt' as top 1\n",
      "INFO: Epoch 11, global step 1200: 'train_loss' reached -2.94396 (best -2.94396), saving model to '/home/aw82/StatML/enhancedts-transformers/scripts/EXP-LongForecasting/lightning_logs/version_3/checkpoints/epoch=11-step=1200.ckpt' as top 1\n",
      "INFO:lightning.pytorch.utilities.rank_zero:Epoch 11, global step 1200: 'train_loss' reached -2.94396 (best -2.94396), saving model to '/home/aw82/StatML/enhancedts-transformers/scripts/EXP-LongForecasting/lightning_logs/version_3/checkpoints/epoch=11-step=1200.ckpt' as top 1\n",
      "INFO: Epoch 12, global step 1300: 'train_loss' was not in top 1\n",
      "INFO:lightning.pytorch.utilities.rank_zero:Epoch 12, global step 1300: 'train_loss' was not in top 1\n",
      "INFO: Epoch 13, global step 1400: 'train_loss' reached -2.94544 (best -2.94544), saving model to '/home/aw82/StatML/enhancedts-transformers/scripts/EXP-LongForecasting/lightning_logs/version_3/checkpoints/epoch=13-step=1400.ckpt' as top 1\n",
      "INFO:lightning.pytorch.utilities.rank_zero:Epoch 13, global step 1400: 'train_loss' reached -2.94544 (best -2.94544), saving model to '/home/aw82/StatML/enhancedts-transformers/scripts/EXP-LongForecasting/lightning_logs/version_3/checkpoints/epoch=13-step=1400.ckpt' as top 1\n",
      "INFO: Epoch 14, global step 1500: 'train_loss' reached -2.95074 (best -2.95074), saving model to '/home/aw82/StatML/enhancedts-transformers/scripts/EXP-LongForecasting/lightning_logs/version_3/checkpoints/epoch=14-step=1500.ckpt' as top 1\n",
      "INFO:lightning.pytorch.utilities.rank_zero:Epoch 14, global step 1500: 'train_loss' reached -2.95074 (best -2.95074), saving model to '/home/aw82/StatML/enhancedts-transformers/scripts/EXP-LongForecasting/lightning_logs/version_3/checkpoints/epoch=14-step=1500.ckpt' as top 1\n",
      "INFO: Epoch 15, global step 1600: 'train_loss' was not in top 1\n",
      "INFO:lightning.pytorch.utilities.rank_zero:Epoch 15, global step 1600: 'train_loss' was not in top 1\n",
      "INFO: Epoch 16, global step 1700: 'train_loss' was not in top 1\n",
      "INFO:lightning.pytorch.utilities.rank_zero:Epoch 16, global step 1700: 'train_loss' was not in top 1\n",
      "INFO: Epoch 17, global step 1800: 'train_loss' reached -2.96189 (best -2.96189), saving model to '/home/aw82/StatML/enhancedts-transformers/scripts/EXP-LongForecasting/lightning_logs/version_3/checkpoints/epoch=17-step=1800.ckpt' as top 1\n",
      "INFO:lightning.pytorch.utilities.rank_zero:Epoch 17, global step 1800: 'train_loss' reached -2.96189 (best -2.96189), saving model to '/home/aw82/StatML/enhancedts-transformers/scripts/EXP-LongForecasting/lightning_logs/version_3/checkpoints/epoch=17-step=1800.ckpt' as top 1\n",
      "INFO: Epoch 18, global step 1900: 'train_loss' was not in top 1\n",
      "INFO:lightning.pytorch.utilities.rank_zero:Epoch 18, global step 1900: 'train_loss' was not in top 1\n",
      "INFO: Epoch 19, global step 2000: 'train_loss' reached -2.96916 (best -2.96916), saving model to '/home/aw82/StatML/enhancedts-transformers/scripts/EXP-LongForecasting/lightning_logs/version_3/checkpoints/epoch=19-step=2000.ckpt' as top 1\n",
      "INFO:lightning.pytorch.utilities.rank_zero:Epoch 19, global step 2000: 'train_loss' reached -2.96916 (best -2.96916), saving model to '/home/aw82/StatML/enhancedts-transformers/scripts/EXP-LongForecasting/lightning_logs/version_3/checkpoints/epoch=19-step=2000.ckpt' as top 1\n",
      "INFO: Epoch 20, global step 2100: 'train_loss' was not in top 1\n",
      "INFO:lightning.pytorch.utilities.rank_zero:Epoch 20, global step 2100: 'train_loss' was not in top 1\n",
      "INFO: Epoch 21, global step 2200: 'train_loss' was not in top 1\n",
      "INFO:lightning.pytorch.utilities.rank_zero:Epoch 21, global step 2200: 'train_loss' was not in top 1\n",
      "INFO: Epoch 22, global step 2300: 'train_loss' was not in top 1\n",
      "INFO:lightning.pytorch.utilities.rank_zero:Epoch 22, global step 2300: 'train_loss' was not in top 1\n",
      "INFO: Epoch 23, global step 2400: 'train_loss' reached -2.97423 (best -2.97423), saving model to '/home/aw82/StatML/enhancedts-transformers/scripts/EXP-LongForecasting/lightning_logs/version_3/checkpoints/epoch=23-step=2400.ckpt' as top 1\n",
      "INFO:lightning.pytorch.utilities.rank_zero:Epoch 23, global step 2400: 'train_loss' reached -2.97423 (best -2.97423), saving model to '/home/aw82/StatML/enhancedts-transformers/scripts/EXP-LongForecasting/lightning_logs/version_3/checkpoints/epoch=23-step=2400.ckpt' as top 1\n",
      "INFO: Epoch 24, global step 2500: 'train_loss' was not in top 1\n",
      "INFO:lightning.pytorch.utilities.rank_zero:Epoch 24, global step 2500: 'train_loss' was not in top 1\n",
      "INFO: Epoch 25, global step 2600: 'train_loss' was not in top 1\n",
      "INFO:lightning.pytorch.utilities.rank_zero:Epoch 25, global step 2600: 'train_loss' was not in top 1\n",
      "INFO: Epoch 26, global step 2700: 'train_loss' was not in top 1\n",
      "INFO:lightning.pytorch.utilities.rank_zero:Epoch 26, global step 2700: 'train_loss' was not in top 1\n",
      "INFO: Epoch 27, global step 2800: 'train_loss' was not in top 1\n",
      "INFO:lightning.pytorch.utilities.rank_zero:Epoch 27, global step 2800: 'train_loss' was not in top 1\n",
      "INFO: Epoch 28, global step 2900: 'train_loss' reached -2.97463 (best -2.97463), saving model to '/home/aw82/StatML/enhancedts-transformers/scripts/EXP-LongForecasting/lightning_logs/version_3/checkpoints/epoch=28-step=2900.ckpt' as top 1\n",
      "INFO:lightning.pytorch.utilities.rank_zero:Epoch 28, global step 2900: 'train_loss' reached -2.97463 (best -2.97463), saving model to '/home/aw82/StatML/enhancedts-transformers/scripts/EXP-LongForecasting/lightning_logs/version_3/checkpoints/epoch=28-step=2900.ckpt' as top 1\n",
      "INFO: Epoch 29, global step 3000: 'train_loss' was not in top 1\n",
      "INFO:lightning.pytorch.utilities.rank_zero:Epoch 29, global step 3000: 'train_loss' was not in top 1\n",
      "INFO: Epoch 30, global step 3100: 'train_loss' was not in top 1\n",
      "INFO:lightning.pytorch.utilities.rank_zero:Epoch 30, global step 3100: 'train_loss' was not in top 1\n",
      "INFO: Epoch 31, global step 3200: 'train_loss' was not in top 1\n",
      "INFO:lightning.pytorch.utilities.rank_zero:Epoch 31, global step 3200: 'train_loss' was not in top 1\n",
      "INFO: Epoch 32, global step 3300: 'train_loss' was not in top 1\n",
      "INFO:lightning.pytorch.utilities.rank_zero:Epoch 32, global step 3300: 'train_loss' was not in top 1\n",
      "INFO: Epoch 33, global step 3400: 'train_loss' was not in top 1\n",
      "INFO:lightning.pytorch.utilities.rank_zero:Epoch 33, global step 3400: 'train_loss' was not in top 1\n",
      "INFO: Epoch 34, global step 3500: 'train_loss' was not in top 1\n",
      "INFO:lightning.pytorch.utilities.rank_zero:Epoch 34, global step 3500: 'train_loss' was not in top 1\n",
      "INFO: Epoch 35, global step 3600: 'train_loss' was not in top 1\n",
      "INFO:lightning.pytorch.utilities.rank_zero:Epoch 35, global step 3600: 'train_loss' was not in top 1\n",
      "INFO: Epoch 36, global step 3700: 'train_loss' was not in top 1\n",
      "INFO:lightning.pytorch.utilities.rank_zero:Epoch 36, global step 3700: 'train_loss' was not in top 1\n",
      "INFO: Epoch 37, global step 3800: 'train_loss' was not in top 1\n",
      "INFO:lightning.pytorch.utilities.rank_zero:Epoch 37, global step 3800: 'train_loss' was not in top 1\n",
      "INFO: Epoch 38, global step 3900: 'train_loss' was not in top 1\n",
      "INFO:lightning.pytorch.utilities.rank_zero:Epoch 38, global step 3900: 'train_loss' was not in top 1\n",
      "INFO: Epoch 39, global step 4000: 'train_loss' reached -2.98064 (best -2.98064), saving model to '/home/aw82/StatML/enhancedts-transformers/scripts/EXP-LongForecasting/lightning_logs/version_3/checkpoints/epoch=39-step=4000.ckpt' as top 1\n",
      "INFO:lightning.pytorch.utilities.rank_zero:Epoch 39, global step 4000: 'train_loss' reached -2.98064 (best -2.98064), saving model to '/home/aw82/StatML/enhancedts-transformers/scripts/EXP-LongForecasting/lightning_logs/version_3/checkpoints/epoch=39-step=4000.ckpt' as top 1\n",
      "INFO: Epoch 40, global step 4100: 'train_loss' was not in top 1\n",
      "INFO:lightning.pytorch.utilities.rank_zero:Epoch 40, global step 4100: 'train_loss' was not in top 1\n",
      "INFO: Epoch 41, global step 4200: 'train_loss' was not in top 1\n",
      "INFO:lightning.pytorch.utilities.rank_zero:Epoch 41, global step 4200: 'train_loss' was not in top 1\n",
      "INFO: Epoch 42, global step 4300: 'train_loss' was not in top 1\n",
      "INFO:lightning.pytorch.utilities.rank_zero:Epoch 42, global step 4300: 'train_loss' was not in top 1\n",
      "INFO: Epoch 43, global step 4400: 'train_loss' was not in top 1\n",
      "INFO:lightning.pytorch.utilities.rank_zero:Epoch 43, global step 4400: 'train_loss' was not in top 1\n",
      "INFO: Epoch 44, global step 4500: 'train_loss' was not in top 1\n",
      "INFO:lightning.pytorch.utilities.rank_zero:Epoch 44, global step 4500: 'train_loss' was not in top 1\n",
      "INFO: Epoch 45, global step 4600: 'train_loss' was not in top 1\n",
      "INFO:lightning.pytorch.utilities.rank_zero:Epoch 45, global step 4600: 'train_loss' was not in top 1\n",
      "INFO: Epoch 46, global step 4700: 'train_loss' was not in top 1\n",
      "INFO:lightning.pytorch.utilities.rank_zero:Epoch 46, global step 4700: 'train_loss' was not in top 1\n",
      "INFO: Epoch 47, global step 4800: 'train_loss' was not in top 1\n",
      "INFO:lightning.pytorch.utilities.rank_zero:Epoch 47, global step 4800: 'train_loss' was not in top 1\n",
      "INFO: Epoch 48, global step 4900: 'train_loss' was not in top 1\n",
      "INFO:lightning.pytorch.utilities.rank_zero:Epoch 48, global step 4900: 'train_loss' was not in top 1\n",
      "INFO: Epoch 49, global step 5000: 'train_loss' was not in top 1\n",
      "INFO:lightning.pytorch.utilities.rank_zero:Epoch 49, global step 5000: 'train_loss' was not in top 1\n",
      "INFO: `Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "INFO:lightning.pytorch.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=50` reached.\n"
     ]
    }
   ],
   "source": [
    "predictor = estimator.train(\n",
    "    training_data=train_dataset, \n",
    "    cache_data=True, \n",
    "    shuffle_buffer_length=1024\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running evaluation: 6034it [00:15, 386.89it/s]\n",
      "/home/aw82/miniconda3/lib/python3.9/site-packages/gluonts/evaluation/_base.py:425: RuntimeWarning: divide by zero encountered in float_scalars\n",
      "  metrics[\"ND\"] = cast(float, metrics[\"abs_error\"]) / cast(\n",
      "/home/aw82/miniconda3/lib/python3.9/site-packages/gluonts/evaluation/_base.py:425: RuntimeWarning: divide by zero encountered in float_scalars\n",
      "  metrics[\"ND\"] = cast(float, metrics[\"abs_error\"]) / cast(\n",
      "Process SpawnPoolWorker-364:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/queues.py\", line 368, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/multiprocessing/reductions.py\", line 121, in rebuild_cuda_tensor\n",
      "    storage = storage_cls._new_shared_cuda(\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/storage.py\", line 807, in _new_shared_cuda\n",
      "    return torch.UntypedStorage._new_shared_cuda(*args, **kwargs)\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Process SpawnPoolWorker-365:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/queues.py\", line 368, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/multiprocessing/reductions.py\", line 121, in rebuild_cuda_tensor\n",
      "    storage = storage_cls._new_shared_cuda(\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/storage.py\", line 807, in _new_shared_cuda\n",
      "    return torch.UntypedStorage._new_shared_cuda(*args, **kwargs)\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Process SpawnPoolWorker-323:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/queues.py\", line 368, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/multiprocessing/reductions.py\", line 121, in rebuild_cuda_tensor\n",
      "    storage = storage_cls._new_shared_cuda(\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/storage.py\", line 807, in _new_shared_cuda\n",
      "    return torch.UntypedStorage._new_shared_cuda(*args, **kwargs)\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Process SpawnPoolWorker-362:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/queues.py\", line 368, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/multiprocessing/reductions.py\", line 121, in rebuild_cuda_tensor\n",
      "    storage = storage_cls._new_shared_cuda(\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/storage.py\", line 807, in _new_shared_cuda\n",
      "    return torch.UntypedStorage._new_shared_cuda(*args, **kwargs)\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Process SpawnPoolWorker-370:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/queues.py\", line 368, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/multiprocessing/reductions.py\", line 121, in rebuild_cuda_tensor\n",
      "    storage = storage_cls._new_shared_cuda(\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/storage.py\", line 807, in _new_shared_cuda\n",
      "    return torch.UntypedStorage._new_shared_cuda(*args, **kwargs)\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Process SpawnPoolWorker-341:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/queues.py\", line 368, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/multiprocessing/reductions.py\", line 121, in rebuild_cuda_tensor\n",
      "    storage = storage_cls._new_shared_cuda(\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/storage.py\", line 807, in _new_shared_cuda\n",
      "    return torch.UntypedStorage._new_shared_cuda(*args, **kwargs)\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Process SpawnPoolWorker-359:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/queues.py\", line 368, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/multiprocessing/reductions.py\", line 121, in rebuild_cuda_tensor\n",
      "    storage = storage_cls._new_shared_cuda(\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/storage.py\", line 807, in _new_shared_cuda\n",
      "    return torch.UntypedStorage._new_shared_cuda(*args, **kwargs)\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Process SpawnPoolWorker-361:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/queues.py\", line 368, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/multiprocessing/reductions.py\", line 121, in rebuild_cuda_tensor\n",
      "    storage = storage_cls._new_shared_cuda(\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/storage.py\", line 807, in _new_shared_cuda\n",
      "    return torch.UntypedStorage._new_shared_cuda(*args, **kwargs)\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Process SpawnPoolWorker-328:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/queues.py\", line 368, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/multiprocessing/reductions.py\", line 121, in rebuild_cuda_tensor\n",
      "    storage = storage_cls._new_shared_cuda(\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/storage.py\", line 807, in _new_shared_cuda\n",
      "    return torch.UntypedStorage._new_shared_cuda(*args, **kwargs)\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Process SpawnPoolWorker-334:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/queues.py\", line 368, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/multiprocessing/reductions.py\", line 121, in rebuild_cuda_tensor\n",
      "    storage = storage_cls._new_shared_cuda(\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/storage.py\", line 807, in _new_shared_cuda\n",
      "    return torch.UntypedStorage._new_shared_cuda(*args, **kwargs)\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Process SpawnPoolWorker-346:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/queues.py\", line 368, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/multiprocessing/reductions.py\", line 121, in rebuild_cuda_tensor\n",
      "    storage = storage_cls._new_shared_cuda(\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/storage.py\", line 807, in _new_shared_cuda\n",
      "    return torch.UntypedStorage._new_shared_cuda(*args, **kwargs)\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Process SpawnPoolWorker-347:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/queues.py\", line 368, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/multiprocessing/reductions.py\", line 121, in rebuild_cuda_tensor\n",
      "    storage = storage_cls._new_shared_cuda(\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/storage.py\", line 807, in _new_shared_cuda\n",
      "    return torch.UntypedStorage._new_shared_cuda(*args, **kwargs)\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Process SpawnPoolWorker-345:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/queues.py\", line 368, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/multiprocessing/reductions.py\", line 121, in rebuild_cuda_tensor\n",
      "    storage = storage_cls._new_shared_cuda(\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/storage.py\", line 807, in _new_shared_cuda\n",
      "    return torch.UntypedStorage._new_shared_cuda(*args, **kwargs)\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Process SpawnPoolWorker-355:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/queues.py\", line 368, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/multiprocessing/reductions.py\", line 121, in rebuild_cuda_tensor\n",
      "    storage = storage_cls._new_shared_cuda(\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/storage.py\", line 807, in _new_shared_cuda\n",
      "    return torch.UntypedStorage._new_shared_cuda(*args, **kwargs)\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Process SpawnPoolWorker-352:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/queues.py\", line 368, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/multiprocessing/reductions.py\", line 121, in rebuild_cuda_tensor\n",
      "    storage = storage_cls._new_shared_cuda(\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/storage.py\", line 807, in _new_shared_cuda\n",
      "    return torch.UntypedStorage._new_shared_cuda(*args, **kwargs)\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Process SpawnPoolWorker-368:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/queues.py\", line 368, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/multiprocessing/reductions.py\", line 121, in rebuild_cuda_tensor\n",
      "    storage = storage_cls._new_shared_cuda(\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/storage.py\", line 807, in _new_shared_cuda\n",
      "    return torch.UntypedStorage._new_shared_cuda(*args, **kwargs)\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Process SpawnPoolWorker-369:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/queues.py\", line 368, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/multiprocessing/reductions.py\", line 121, in rebuild_cuda_tensor\n",
      "    storage = storage_cls._new_shared_cuda(\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/storage.py\", line 807, in _new_shared_cuda\n",
      "    return torch.UntypedStorage._new_shared_cuda(*args, **kwargs)\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Process SpawnPoolWorker-367:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/queues.py\", line 368, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/multiprocessing/reductions.py\", line 121, in rebuild_cuda_tensor\n",
      "    storage = storage_cls._new_shared_cuda(\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/storage.py\", line 807, in _new_shared_cuda\n",
      "    return torch.UntypedStorage._new_shared_cuda(*args, **kwargs)\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Process SpawnPoolWorker-358:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/queues.py\", line 368, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/multiprocessing/reductions.py\", line 121, in rebuild_cuda_tensor\n",
      "    storage = storage_cls._new_shared_cuda(\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/storage.py\", line 807, in _new_shared_cuda\n",
      "    return torch.UntypedStorage._new_shared_cuda(*args, **kwargs)\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Process SpawnPoolWorker-366:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/queues.py\", line 368, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/multiprocessing/reductions.py\", line 121, in rebuild_cuda_tensor\n",
      "    storage = storage_cls._new_shared_cuda(\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/storage.py\", line 807, in _new_shared_cuda\n",
      "    return torch.UntypedStorage._new_shared_cuda(*args, **kwargs)\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Process SpawnPoolWorker-289:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/queues.py\", line 368, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/multiprocessing/reductions.py\", line 121, in rebuild_cuda_tensor\n",
      "    storage = storage_cls._new_shared_cuda(\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/storage.py\", line 807, in _new_shared_cuda\n",
      "    return torch.UntypedStorage._new_shared_cuda(*args, **kwargs)\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Process SpawnPoolWorker-288:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/queues.py\", line 368, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/multiprocessing/reductions.py\", line 121, in rebuild_cuda_tensor\n",
      "    storage = storage_cls._new_shared_cuda(\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/storage.py\", line 807, in _new_shared_cuda\n",
      "    return torch.UntypedStorage._new_shared_cuda(*args, **kwargs)\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Process SpawnPoolWorker-315:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/queues.py\", line 368, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/multiprocessing/reductions.py\", line 121, in rebuild_cuda_tensor\n",
      "    storage = storage_cls._new_shared_cuda(\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/storage.py\", line 807, in _new_shared_cuda\n",
      "    return torch.UntypedStorage._new_shared_cuda(*args, **kwargs)\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Process SpawnPoolWorker-331:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/queues.py\", line 368, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/multiprocessing/reductions.py\", line 121, in rebuild_cuda_tensor\n",
      "    storage = storage_cls._new_shared_cuda(\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/storage.py\", line 807, in _new_shared_cuda\n",
      "    return torch.UntypedStorage._new_shared_cuda(*args, **kwargs)\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Process SpawnPoolWorker-353:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/queues.py\", line 368, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/multiprocessing/reductions.py\", line 121, in rebuild_cuda_tensor\n",
      "    storage = storage_cls._new_shared_cuda(\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/storage.py\", line 807, in _new_shared_cuda\n",
      "    return torch.UntypedStorage._new_shared_cuda(*args, **kwargs)\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Process SpawnPoolWorker-304:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/queues.py\", line 368, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/multiprocessing/reductions.py\", line 121, in rebuild_cuda_tensor\n",
      "    storage = storage_cls._new_shared_cuda(\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/storage.py\", line 807, in _new_shared_cuda\n",
      "    return torch.UntypedStorage._new_shared_cuda(*args, **kwargs)\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Process SpawnPoolWorker-280:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/queues.py\", line 368, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/multiprocessing/reductions.py\", line 121, in rebuild_cuda_tensor\n",
      "    storage = storage_cls._new_shared_cuda(\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/storage.py\", line 807, in _new_shared_cuda\n",
      "    return torch.UntypedStorage._new_shared_cuda(*args, **kwargs)\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Process SpawnPoolWorker-294:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/queues.py\", line 368, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/multiprocessing/reductions.py\", line 121, in rebuild_cuda_tensor\n",
      "    storage = storage_cls._new_shared_cuda(\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/storage.py\", line 807, in _new_shared_cuda\n",
      "    return torch.UntypedStorage._new_shared_cuda(*args, **kwargs)\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Process SpawnPoolWorker-257:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/queues.py\", line 368, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/multiprocessing/reductions.py\", line 121, in rebuild_cuda_tensor\n",
      "    storage = storage_cls._new_shared_cuda(\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/storage.py\", line 807, in _new_shared_cuda\n",
      "    return torch.UntypedStorage._new_shared_cuda(*args, **kwargs)\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Process SpawnPoolWorker-282:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/queues.py\", line 368, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/multiprocessing/reductions.py\", line 121, in rebuild_cuda_tensor\n",
      "    storage = storage_cls._new_shared_cuda(\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/storage.py\", line 807, in _new_shared_cuda\n",
      "    return torch.UntypedStorage._new_shared_cuda(*args, **kwargs)\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Process SpawnPoolWorker-322:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/queues.py\", line 368, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/multiprocessing/reductions.py\", line 121, in rebuild_cuda_tensor\n",
      "    storage = storage_cls._new_shared_cuda(\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/storage.py\", line 807, in _new_shared_cuda\n",
      "    return torch.UntypedStorage._new_shared_cuda(*args, **kwargs)\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Process SpawnPoolWorker-312:\n",
      "Process SpawnPoolWorker-348:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/queues.py\", line 368, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/multiprocessing/reductions.py\", line 121, in rebuild_cuda_tensor\n",
      "    storage = storage_cls._new_shared_cuda(\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/storage.py\", line 807, in _new_shared_cuda\n",
      "    return torch.UntypedStorage._new_shared_cuda(*args, **kwargs)\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/queues.py\", line 368, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/multiprocessing/reductions.py\", line 121, in rebuild_cuda_tensor\n",
      "    storage = storage_cls._new_shared_cuda(\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/storage.py\", line 807, in _new_shared_cuda\n",
      "    return torch.UntypedStorage._new_shared_cuda(*args, **kwargs)\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Process SpawnPoolWorker-301:\n",
      "Process SpawnPoolWorker-330:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/queues.py\", line 368, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/multiprocessing/reductions.py\", line 121, in rebuild_cuda_tensor\n",
      "    storage = storage_cls._new_shared_cuda(\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/storage.py\", line 807, in _new_shared_cuda\n",
      "    return torch.UntypedStorage._new_shared_cuda(*args, **kwargs)\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/queues.py\", line 368, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/multiprocessing/reductions.py\", line 121, in rebuild_cuda_tensor\n",
      "    storage = storage_cls._new_shared_cuda(\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/storage.py\", line 807, in _new_shared_cuda\n",
      "    return torch.UntypedStorage._new_shared_cuda(*args, **kwargs)\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Process SpawnPoolWorker-256:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/queues.py\", line 368, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/multiprocessing/reductions.py\", line 121, in rebuild_cuda_tensor\n",
      "    storage = storage_cls._new_shared_cuda(\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/storage.py\", line 807, in _new_shared_cuda\n",
      "    return torch.UntypedStorage._new_shared_cuda(*args, **kwargs)\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Process SpawnPoolWorker-274:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/queues.py\", line 368, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/multiprocessing/reductions.py\", line 121, in rebuild_cuda_tensor\n",
      "    storage = storage_cls._new_shared_cuda(\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/storage.py\", line 807, in _new_shared_cuda\n",
      "    return torch.UntypedStorage._new_shared_cuda(*args, **kwargs)\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Process SpawnPoolWorker-317:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/queues.py\", line 368, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/multiprocessing/reductions.py\", line 121, in rebuild_cuda_tensor\n",
      "    storage = storage_cls._new_shared_cuda(\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/storage.py\", line 807, in _new_shared_cuda\n",
      "    return torch.UntypedStorage._new_shared_cuda(*args, **kwargs)\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Process SpawnPoolWorker-363:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/queues.py\", line 368, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/multiprocessing/reductions.py\", line 121, in rebuild_cuda_tensor\n",
      "    storage = storage_cls._new_shared_cuda(\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/storage.py\", line 807, in _new_shared_cuda\n",
      "    return torch.UntypedStorage._new_shared_cuda(*args, **kwargs)\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Process SpawnPoolWorker-321:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/queues.py\", line 368, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/multiprocessing/reductions.py\", line 121, in rebuild_cuda_tensor\n",
      "    storage = storage_cls._new_shared_cuda(\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/storage.py\", line 807, in _new_shared_cuda\n",
      "    return torch.UntypedStorage._new_shared_cuda(*args, **kwargs)\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Process SpawnPoolWorker-265:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/queues.py\", line 368, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/multiprocessing/reductions.py\", line 121, in rebuild_cuda_tensor\n",
      "    storage = storage_cls._new_shared_cuda(\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/storage.py\", line 807, in _new_shared_cuda\n",
      "    return torch.UntypedStorage._new_shared_cuda(*args, **kwargs)\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Process SpawnPoolWorker-277:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/queues.py\", line 368, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/multiprocessing/reductions.py\", line 121, in rebuild_cuda_tensor\n",
      "    storage = storage_cls._new_shared_cuda(\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/storage.py\", line 807, in _new_shared_cuda\n",
      "    return torch.UntypedStorage._new_shared_cuda(*args, **kwargs)\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Process SpawnPoolWorker-308:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/queues.py\", line 368, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/multiprocessing/reductions.py\", line 121, in rebuild_cuda_tensor\n",
      "    storage = storage_cls._new_shared_cuda(\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/storage.py\", line 807, in _new_shared_cuda\n",
      "    return torch.UntypedStorage._new_shared_cuda(*args, **kwargs)\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Process SpawnPoolWorker-284:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/queues.py\", line 368, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/multiprocessing/reductions.py\", line 121, in rebuild_cuda_tensor\n",
      "    storage = storage_cls._new_shared_cuda(\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/storage.py\", line 807, in _new_shared_cuda\n",
      "    return torch.UntypedStorage._new_shared_cuda(*args, **kwargs)\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Process SpawnPoolWorker-339:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/queues.py\", line 368, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/multiprocessing/reductions.py\", line 121, in rebuild_cuda_tensor\n",
      "    storage = storage_cls._new_shared_cuda(\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/storage.py\", line 807, in _new_shared_cuda\n",
      "    return torch.UntypedStorage._new_shared_cuda(*args, **kwargs)\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Process SpawnPoolWorker-268:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/queues.py\", line 368, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/multiprocessing/reductions.py\", line 121, in rebuild_cuda_tensor\n",
      "    storage = storage_cls._new_shared_cuda(\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/storage.py\", line 807, in _new_shared_cuda\n",
      "    return torch.UntypedStorage._new_shared_cuda(*args, **kwargs)\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Process SpawnPoolWorker-305:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/queues.py\", line 368, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/multiprocessing/reductions.py\", line 121, in rebuild_cuda_tensor\n",
      "    storage = storage_cls._new_shared_cuda(\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/storage.py\", line 807, in _new_shared_cuda\n",
      "    return torch.UntypedStorage._new_shared_cuda(*args, **kwargs)\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Process SpawnPoolWorker-283:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/queues.py\", line 368, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/multiprocessing/reductions.py\", line 121, in rebuild_cuda_tensor\n",
      "    storage = storage_cls._new_shared_cuda(\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/storage.py\", line 807, in _new_shared_cuda\n",
      "    return torch.UntypedStorage._new_shared_cuda(*args, **kwargs)\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Process SpawnPoolWorker-318:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/queues.py\", line 368, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/multiprocessing/reductions.py\", line 121, in rebuild_cuda_tensor\n",
      "    storage = storage_cls._new_shared_cuda(\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/storage.py\", line 807, in _new_shared_cuda\n",
      "    return torch.UntypedStorage._new_shared_cuda(*args, **kwargs)\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Process SpawnPoolWorker-343:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/queues.py\", line 368, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/multiprocessing/reductions.py\", line 121, in rebuild_cuda_tensor\n",
      "    storage = storage_cls._new_shared_cuda(\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/storage.py\", line 807, in _new_shared_cuda\n",
      "    return torch.UntypedStorage._new_shared_cuda(*args, **kwargs)\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Process SpawnPoolWorker-337:\n",
      "Process SpawnPoolWorker-299:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/queues.py\", line 368, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/multiprocessing/reductions.py\", line 121, in rebuild_cuda_tensor\n",
      "    storage = storage_cls._new_shared_cuda(\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/storage.py\", line 807, in _new_shared_cuda\n",
      "    return torch.UntypedStorage._new_shared_cuda(*args, **kwargs)\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/queues.py\", line 368, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/multiprocessing/reductions.py\", line 121, in rebuild_cuda_tensor\n",
      "    storage = storage_cls._new_shared_cuda(\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/storage.py\", line 807, in _new_shared_cuda\n",
      "    return torch.UntypedStorage._new_shared_cuda(*args, **kwargs)\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Process SpawnPoolWorker-269:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/queues.py\", line 368, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/multiprocessing/reductions.py\", line 121, in rebuild_cuda_tensor\n",
      "    storage = storage_cls._new_shared_cuda(\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/storage.py\", line 807, in _new_shared_cuda\n",
      "    return torch.UntypedStorage._new_shared_cuda(*args, **kwargs)\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Process SpawnPoolWorker-287:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/queues.py\", line 368, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/multiprocessing/reductions.py\", line 121, in rebuild_cuda_tensor\n",
      "    storage = storage_cls._new_shared_cuda(\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/storage.py\", line 807, in _new_shared_cuda\n",
      "    return torch.UntypedStorage._new_shared_cuda(*args, **kwargs)\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Process SpawnPoolWorker-338:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/queues.py\", line 368, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/multiprocessing/reductions.py\", line 121, in rebuild_cuda_tensor\n",
      "    storage = storage_cls._new_shared_cuda(\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/storage.py\", line 807, in _new_shared_cuda\n",
      "    return torch.UntypedStorage._new_shared_cuda(*args, **kwargs)\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Process SpawnPoolWorker-336:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/queues.py\", line 368, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/multiprocessing/reductions.py\", line 121, in rebuild_cuda_tensor\n",
      "    storage = storage_cls._new_shared_cuda(\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/storage.py\", line 807, in _new_shared_cuda\n",
      "    return torch.UntypedStorage._new_shared_cuda(*args, **kwargs)\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Process SpawnPoolWorker-291:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/queues.py\", line 368, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/multiprocessing/reductions.py\", line 121, in rebuild_cuda_tensor\n",
      "    storage = storage_cls._new_shared_cuda(\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/storage.py\", line 807, in _new_shared_cuda\n",
      "    return torch.UntypedStorage._new_shared_cuda(*args, **kwargs)\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Process SpawnPoolWorker-278:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/queues.py\", line 368, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/multiprocessing/reductions.py\", line 121, in rebuild_cuda_tensor\n",
      "    storage = storage_cls._new_shared_cuda(\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/storage.py\", line 807, in _new_shared_cuda\n",
      "    return torch.UntypedStorage._new_shared_cuda(*args, **kwargs)\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Process SpawnPoolWorker-285:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/queues.py\", line 368, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/multiprocessing/reductions.py\", line 121, in rebuild_cuda_tensor\n",
      "    storage = storage_cls._new_shared_cuda(\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/storage.py\", line 807, in _new_shared_cuda\n",
      "    return torch.UntypedStorage._new_shared_cuda(*args, **kwargs)\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Process SpawnPoolWorker-327:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/queues.py\", line 368, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/multiprocessing/reductions.py\", line 121, in rebuild_cuda_tensor\n",
      "    storage = storage_cls._new_shared_cuda(\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/storage.py\", line 807, in _new_shared_cuda\n",
      "    return torch.UntypedStorage._new_shared_cuda(*args, **kwargs)\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Process SpawnPoolWorker-335:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/queues.py\", line 368, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/multiprocessing/reductions.py\", line 121, in rebuild_cuda_tensor\n",
      "    storage = storage_cls._new_shared_cuda(\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/storage.py\", line 807, in _new_shared_cuda\n",
      "    return torch.UntypedStorage._new_shared_cuda(*args, **kwargs)\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Process SpawnPoolWorker-320:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/queues.py\", line 368, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/multiprocessing/reductions.py\", line 121, in rebuild_cuda_tensor\n",
      "    storage = storage_cls._new_shared_cuda(\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/storage.py\", line 807, in _new_shared_cuda\n",
      "    return torch.UntypedStorage._new_shared_cuda(*args, **kwargs)\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Process SpawnPoolWorker-324:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/queues.py\", line 368, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/multiprocessing/reductions.py\", line 121, in rebuild_cuda_tensor\n",
      "    storage = storage_cls._new_shared_cuda(\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/storage.py\", line 807, in _new_shared_cuda\n",
      "    return torch.UntypedStorage._new_shared_cuda(*args, **kwargs)\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Process SpawnPoolWorker-293:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/queues.py\", line 368, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/multiprocessing/reductions.py\", line 121, in rebuild_cuda_tensor\n",
      "    storage = storage_cls._new_shared_cuda(\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/storage.py\", line 807, in _new_shared_cuda\n",
      "    return torch.UntypedStorage._new_shared_cuda(*args, **kwargs)\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Process SpawnPoolWorker-329:\n",
      "Process SpawnPoolWorker-270:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/queues.py\", line 368, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/multiprocessing/reductions.py\", line 121, in rebuild_cuda_tensor\n",
      "    storage = storage_cls._new_shared_cuda(\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/storage.py\", line 807, in _new_shared_cuda\n",
      "    return torch.UntypedStorage._new_shared_cuda(*args, **kwargs)\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/queues.py\", line 368, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/multiprocessing/reductions.py\", line 121, in rebuild_cuda_tensor\n",
      "    storage = storage_cls._new_shared_cuda(\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/storage.py\", line 807, in _new_shared_cuda\n",
      "    return torch.UntypedStorage._new_shared_cuda(*args, **kwargs)\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Process SpawnPoolWorker-292:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/queues.py\", line 368, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/multiprocessing/reductions.py\", line 121, in rebuild_cuda_tensor\n",
      "    storage = storage_cls._new_shared_cuda(\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/storage.py\", line 807, in _new_shared_cuda\n",
      "    return torch.UntypedStorage._new_shared_cuda(*args, **kwargs)\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Process SpawnPoolWorker-302:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/queues.py\", line 368, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/multiprocessing/reductions.py\", line 121, in rebuild_cuda_tensor\n",
      "    storage = storage_cls._new_shared_cuda(\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/storage.py\", line 807, in _new_shared_cuda\n",
      "    return torch.UntypedStorage._new_shared_cuda(*args, **kwargs)\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Process SpawnPoolWorker-306:\n",
      "Process SpawnPoolWorker-303:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/queues.py\", line 368, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/multiprocessing/reductions.py\", line 121, in rebuild_cuda_tensor\n",
      "    storage = storage_cls._new_shared_cuda(\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/storage.py\", line 807, in _new_shared_cuda\n",
      "    return torch.UntypedStorage._new_shared_cuda(*args, **kwargs)\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/queues.py\", line 368, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/multiprocessing/reductions.py\", line 121, in rebuild_cuda_tensor\n",
      "    storage = storage_cls._new_shared_cuda(\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/storage.py\", line 807, in _new_shared_cuda\n",
      "    return torch.UntypedStorage._new_shared_cuda(*args, **kwargs)\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Process SpawnPoolWorker-297:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/queues.py\", line 368, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/multiprocessing/reductions.py\", line 121, in rebuild_cuda_tensor\n",
      "    storage = storage_cls._new_shared_cuda(\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/storage.py\", line 807, in _new_shared_cuda\n",
      "    return torch.UntypedStorage._new_shared_cuda(*args, **kwargs)\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Process SpawnPoolWorker-271:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/queues.py\", line 368, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/multiprocessing/reductions.py\", line 121, in rebuild_cuda_tensor\n",
      "    storage = storage_cls._new_shared_cuda(\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/storage.py\", line 807, in _new_shared_cuda\n",
      "    return torch.UntypedStorage._new_shared_cuda(*args, **kwargs)\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Process SpawnPoolWorker-259:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/queues.py\", line 368, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/multiprocessing/reductions.py\", line 121, in rebuild_cuda_tensor\n",
      "    storage = storage_cls._new_shared_cuda(\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/storage.py\", line 807, in _new_shared_cuda\n",
      "    return torch.UntypedStorage._new_shared_cuda(*args, **kwargs)\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Process SpawnPoolWorker-273:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/queues.py\", line 368, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/multiprocessing/reductions.py\", line 121, in rebuild_cuda_tensor\n",
      "    storage = storage_cls._new_shared_cuda(\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/storage.py\", line 807, in _new_shared_cuda\n",
      "    return torch.UntypedStorage._new_shared_cuda(*args, **kwargs)\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Process SpawnPoolWorker-340:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/queues.py\", line 368, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/multiprocessing/reductions.py\", line 121, in rebuild_cuda_tensor\n",
      "    storage = storage_cls._new_shared_cuda(\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/storage.py\", line 807, in _new_shared_cuda\n",
      "    return torch.UntypedStorage._new_shared_cuda(*args, **kwargs)\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Process SpawnPoolWorker-272:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/queues.py\", line 368, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/multiprocessing/reductions.py\", line 121, in rebuild_cuda_tensor\n",
      "    storage = storage_cls._new_shared_cuda(\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/storage.py\", line 807, in _new_shared_cuda\n",
      "    return torch.UntypedStorage._new_shared_cuda(*args, **kwargs)\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Process SpawnPoolWorker-316:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/queues.py\", line 368, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/multiprocessing/reductions.py\", line 121, in rebuild_cuda_tensor\n",
      "    storage = storage_cls._new_shared_cuda(\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/storage.py\", line 807, in _new_shared_cuda\n",
      "    return torch.UntypedStorage._new_shared_cuda(*args, **kwargs)\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Process SpawnPoolWorker-267:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/queues.py\", line 368, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/multiprocessing/reductions.py\", line 121, in rebuild_cuda_tensor\n",
      "    storage = storage_cls._new_shared_cuda(\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/storage.py\", line 807, in _new_shared_cuda\n",
      "    return torch.UntypedStorage._new_shared_cuda(*args, **kwargs)\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Process SpawnPoolWorker-295:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/queues.py\", line 368, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/multiprocessing/reductions.py\", line 121, in rebuild_cuda_tensor\n",
      "    storage = storage_cls._new_shared_cuda(\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/storage.py\", line 807, in _new_shared_cuda\n",
      "    return torch.UntypedStorage._new_shared_cuda(*args, **kwargs)\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Process SpawnPoolWorker-333:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/queues.py\", line 368, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/multiprocessing/reductions.py\", line 121, in rebuild_cuda_tensor\n",
      "    storage = storage_cls._new_shared_cuda(\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/storage.py\", line 807, in _new_shared_cuda\n",
      "    return torch.UntypedStorage._new_shared_cuda(*args, **kwargs)\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Process SpawnPoolWorker-314:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/queues.py\", line 368, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/multiprocessing/reductions.py\", line 121, in rebuild_cuda_tensor\n",
      "    storage = storage_cls._new_shared_cuda(\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/storage.py\", line 807, in _new_shared_cuda\n",
      "    return torch.UntypedStorage._new_shared_cuda(*args, **kwargs)\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Process SpawnPoolWorker-258:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/queues.py\", line 368, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/multiprocessing/reductions.py\", line 121, in rebuild_cuda_tensor\n",
      "    storage = storage_cls._new_shared_cuda(\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/storage.py\", line 807, in _new_shared_cuda\n",
      "    return torch.UntypedStorage._new_shared_cuda(*args, **kwargs)\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Process SpawnPoolWorker-275:\n",
      "Process SpawnPoolWorker-290:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/queues.py\", line 368, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/multiprocessing/reductions.py\", line 121, in rebuild_cuda_tensor\n",
      "    storage = storage_cls._new_shared_cuda(\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/storage.py\", line 807, in _new_shared_cuda\n",
      "    return torch.UntypedStorage._new_shared_cuda(*args, **kwargs)\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/queues.py\", line 368, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/multiprocessing/reductions.py\", line 121, in rebuild_cuda_tensor\n",
      "    storage = storage_cls._new_shared_cuda(\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/storage.py\", line 807, in _new_shared_cuda\n",
      "    return torch.UntypedStorage._new_shared_cuda(*args, **kwargs)\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Process SpawnPoolWorker-300:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/queues.py\", line 368, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/multiprocessing/reductions.py\", line 121, in rebuild_cuda_tensor\n",
      "    storage = storage_cls._new_shared_cuda(\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/storage.py\", line 807, in _new_shared_cuda\n",
      "    return torch.UntypedStorage._new_shared_cuda(*args, **kwargs)\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Process SpawnPoolWorker-307:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/queues.py\", line 368, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/multiprocessing/reductions.py\", line 121, in rebuild_cuda_tensor\n",
      "    storage = storage_cls._new_shared_cuda(\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/storage.py\", line 807, in _new_shared_cuda\n",
      "    return torch.UntypedStorage._new_shared_cuda(*args, **kwargs)\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Process SpawnPoolWorker-313:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/queues.py\", line 368, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/multiprocessing/reductions.py\", line 121, in rebuild_cuda_tensor\n",
      "    storage = storage_cls._new_shared_cuda(\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/storage.py\", line 807, in _new_shared_cuda\n",
      "    return torch.UntypedStorage._new_shared_cuda(*args, **kwargs)\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Process SpawnPoolWorker-262:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/queues.py\", line 368, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/multiprocessing/reductions.py\", line 121, in rebuild_cuda_tensor\n",
      "    storage = storage_cls._new_shared_cuda(\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/storage.py\", line 807, in _new_shared_cuda\n",
      "    return torch.UntypedStorage._new_shared_cuda(*args, **kwargs)\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Process SpawnPoolWorker-325:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/queues.py\", line 368, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/multiprocessing/reductions.py\", line 121, in rebuild_cuda_tensor\n",
      "    storage = storage_cls._new_shared_cuda(\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/storage.py\", line 807, in _new_shared_cuda\n",
      "    return torch.UntypedStorage._new_shared_cuda(*args, **kwargs)\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Process SpawnPoolWorker-261:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/queues.py\", line 368, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/multiprocessing/reductions.py\", line 121, in rebuild_cuda_tensor\n",
      "    storage = storage_cls._new_shared_cuda(\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/storage.py\", line 807, in _new_shared_cuda\n",
      "    return torch.UntypedStorage._new_shared_cuda(*args, **kwargs)\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Process SpawnPoolWorker-342:\n",
      "Process SpawnPoolWorker-263:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/queues.py\", line 368, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/multiprocessing/reductions.py\", line 121, in rebuild_cuda_tensor\n",
      "    storage = storage_cls._new_shared_cuda(\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/storage.py\", line 807, in _new_shared_cuda\n",
      "    return torch.UntypedStorage._new_shared_cuda(*args, **kwargs)\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/queues.py\", line 368, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/multiprocessing/reductions.py\", line 121, in rebuild_cuda_tensor\n",
      "    storage = storage_cls._new_shared_cuda(\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/storage.py\", line 807, in _new_shared_cuda\n",
      "    return torch.UntypedStorage._new_shared_cuda(*args, **kwargs)\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Process SpawnPoolWorker-311:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/queues.py\", line 368, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/multiprocessing/reductions.py\", line 121, in rebuild_cuda_tensor\n",
      "    storage = storage_cls._new_shared_cuda(\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/storage.py\", line 807, in _new_shared_cuda\n",
      "    return torch.UntypedStorage._new_shared_cuda(*args, **kwargs)\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Process SpawnPoolWorker-319:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/queues.py\", line 368, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/multiprocessing/reductions.py\", line 121, in rebuild_cuda_tensor\n",
      "    storage = storage_cls._new_shared_cuda(\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/storage.py\", line 807, in _new_shared_cuda\n",
      "    return torch.UntypedStorage._new_shared_cuda(*args, **kwargs)\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Process SpawnPoolWorker-298:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/queues.py\", line 368, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/multiprocessing/reductions.py\", line 121, in rebuild_cuda_tensor\n",
      "    storage = storage_cls._new_shared_cuda(\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/storage.py\", line 807, in _new_shared_cuda\n",
      "    return torch.UntypedStorage._new_shared_cuda(*args, **kwargs)\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Process SpawnPoolWorker-310:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/queues.py\", line 368, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/multiprocessing/reductions.py\", line 121, in rebuild_cuda_tensor\n",
      "    storage = storage_cls._new_shared_cuda(\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/storage.py\", line 807, in _new_shared_cuda\n",
      "    return torch.UntypedStorage._new_shared_cuda(*args, **kwargs)\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Process SpawnPoolWorker-266:\n",
      "Process SpawnPoolWorker-326:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/queues.py\", line 368, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/multiprocessing/reductions.py\", line 121, in rebuild_cuda_tensor\n",
      "    storage = storage_cls._new_shared_cuda(\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/storage.py\", line 807, in _new_shared_cuda\n",
      "    return torch.UntypedStorage._new_shared_cuda(*args, **kwargs)\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/queues.py\", line 368, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/multiprocessing/reductions.py\", line 121, in rebuild_cuda_tensor\n",
      "    storage = storage_cls._new_shared_cuda(\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/storage.py\", line 807, in _new_shared_cuda\n",
      "    return torch.UntypedStorage._new_shared_cuda(*args, **kwargs)\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Process SpawnPoolWorker-264:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/queues.py\", line 368, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/multiprocessing/reductions.py\", line 121, in rebuild_cuda_tensor\n",
      "    storage = storage_cls._new_shared_cuda(\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/storage.py\", line 807, in _new_shared_cuda\n",
      "    return torch.UntypedStorage._new_shared_cuda(*args, **kwargs)\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Process SpawnPoolWorker-276:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/queues.py\", line 368, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/multiprocessing/reductions.py\", line 121, in rebuild_cuda_tensor\n",
      "    storage = storage_cls._new_shared_cuda(\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/storage.py\", line 807, in _new_shared_cuda\n",
      "    return torch.UntypedStorage._new_shared_cuda(*args, **kwargs)\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Process SpawnPoolWorker-279:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/queues.py\", line 368, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/multiprocessing/reductions.py\", line 121, in rebuild_cuda_tensor\n",
      "    storage = storage_cls._new_shared_cuda(\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/storage.py\", line 807, in _new_shared_cuda\n",
      "    return torch.UntypedStorage._new_shared_cuda(*args, **kwargs)\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Process SpawnPoolWorker-332:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/queues.py\", line 368, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/multiprocessing/reductions.py\", line 121, in rebuild_cuda_tensor\n",
      "    storage = storage_cls._new_shared_cuda(\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/storage.py\", line 807, in _new_shared_cuda\n",
      "    return torch.UntypedStorage._new_shared_cuda(*args, **kwargs)\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Process SpawnPoolWorker-309:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/queues.py\", line 368, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/multiprocessing/reductions.py\", line 121, in rebuild_cuda_tensor\n",
      "    storage = storage_cls._new_shared_cuda(\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/storage.py\", line 807, in _new_shared_cuda\n",
      "    return torch.UntypedStorage._new_shared_cuda(*args, **kwargs)\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Process SpawnPoolWorker-255:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/queues.py\", line 368, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/multiprocessing/reductions.py\", line 121, in rebuild_cuda_tensor\n",
      "    storage = storage_cls._new_shared_cuda(\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/storage.py\", line 807, in _new_shared_cuda\n",
      "    return torch.UntypedStorage._new_shared_cuda(*args, **kwargs)\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Process SpawnPoolWorker-281:\n",
      "Process SpawnPoolWorker-260:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/queues.py\", line 368, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/multiprocessing/reductions.py\", line 121, in rebuild_cuda_tensor\n",
      "    storage = storage_cls._new_shared_cuda(\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/storage.py\", line 807, in _new_shared_cuda\n",
      "    return torch.UntypedStorage._new_shared_cuda(*args, **kwargs)\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/queues.py\", line 368, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/multiprocessing/reductions.py\", line 121, in rebuild_cuda_tensor\n",
      "    storage = storage_cls._new_shared_cuda(\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/storage.py\", line 807, in _new_shared_cuda\n",
      "    return torch.UntypedStorage._new_shared_cuda(*args, **kwargs)\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Process SpawnPoolWorker-344:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/queues.py\", line 368, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/multiprocessing/reductions.py\", line 121, in rebuild_cuda_tensor\n",
      "    storage = storage_cls._new_shared_cuda(\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/storage.py\", line 807, in _new_shared_cuda\n",
      "    return torch.UntypedStorage._new_shared_cuda(*args, **kwargs)\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Process SpawnPoolWorker-296:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/queues.py\", line 368, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/multiprocessing/reductions.py\", line 121, in rebuild_cuda_tensor\n",
      "    storage = storage_cls._new_shared_cuda(\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/storage.py\", line 807, in _new_shared_cuda\n",
      "    return torch.UntypedStorage._new_shared_cuda(*args, **kwargs)\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Process SpawnPoolWorker-286:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/queues.py\", line 368, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/multiprocessing/reductions.py\", line 121, in rebuild_cuda_tensor\n",
      "    storage = storage_cls._new_shared_cuda(\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/storage.py\", line 807, in _new_shared_cuda\n",
      "    return torch.UntypedStorage._new_shared_cuda(*args, **kwargs)\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Process SpawnPoolWorker-356:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/queues.py\", line 368, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/multiprocessing/reductions.py\", line 121, in rebuild_cuda_tensor\n",
      "    storage = storage_cls._new_shared_cuda(\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/storage.py\", line 807, in _new_shared_cuda\n",
      "    return torch.UntypedStorage._new_shared_cuda(*args, **kwargs)\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Process SpawnPoolWorker-360:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/queues.py\", line 368, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/multiprocessing/reductions.py\", line 121, in rebuild_cuda_tensor\n",
      "    storage = storage_cls._new_shared_cuda(\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/storage.py\", line 807, in _new_shared_cuda\n",
      "    return torch.UntypedStorage._new_shared_cuda(*args, **kwargs)\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Process SpawnPoolWorker-357:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/queues.py\", line 368, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/multiprocessing/reductions.py\", line 121, in rebuild_cuda_tensor\n",
      "    storage = storage_cls._new_shared_cuda(\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/storage.py\", line 807, in _new_shared_cuda\n",
      "    return torch.UntypedStorage._new_shared_cuda(*args, **kwargs)\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Process SpawnPoolWorker-351:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/queues.py\", line 368, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/multiprocessing/reductions.py\", line 121, in rebuild_cuda_tensor\n",
      "    storage = storage_cls._new_shared_cuda(\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/storage.py\", line 807, in _new_shared_cuda\n",
      "    return torch.UntypedStorage._new_shared_cuda(*args, **kwargs)\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Process SpawnPoolWorker-349:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/queues.py\", line 368, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/multiprocessing/reductions.py\", line 121, in rebuild_cuda_tensor\n",
      "    storage = storage_cls._new_shared_cuda(\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/storage.py\", line 807, in _new_shared_cuda\n",
      "    return torch.UntypedStorage._new_shared_cuda(*args, **kwargs)\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Process SpawnPoolWorker-350:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/queues.py\", line 368, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/multiprocessing/reductions.py\", line 121, in rebuild_cuda_tensor\n",
      "    storage = storage_cls._new_shared_cuda(\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/storage.py\", line 807, in _new_shared_cuda\n",
      "    return torch.UntypedStorage._new_shared_cuda(*args, **kwargs)\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Process SpawnPoolWorker-354:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/multiprocessing/queues.py\", line 368, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/multiprocessing/reductions.py\", line 121, in rebuild_cuda_tensor\n",
      "    storage = storage_cls._new_shared_cuda(\n",
      "  File \"/home/aw82/miniconda3/lib/python3.9/site-packages/torch/storage.py\", line 807, in _new_shared_cuda\n",
      "    return torch.UntypedStorage._new_shared_cuda(*args, **kwargs)\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "/home/aw82/miniconda3/lib/python3.9/site-packages/gluonts/evaluation/_base.py:425: RuntimeWarning: divide by zero encountered in float_scalars\n",
      "  metrics[\"ND\"] = cast(float, metrics[\"abs_error\"]) / cast(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/aw82/StatML/enhancedts-transformers/scripts/EXP-LongForecasting/hf_script.ipynb Cell 19\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bterminator7.cs.rice.edu/home/aw82/StatML/enhancedts-transformers/scripts/EXP-LongForecasting/hf_script.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# from gluonts.evaluation import make_evaluation_predictions, Evaluator\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bterminator7.cs.rice.edu/home/aw82/StatML/enhancedts-transformers/scripts/EXP-LongForecasting/hf_script.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bterminator7.cs.rice.edu/home/aw82/StatML/enhancedts-transformers/scripts/EXP-LongForecasting/hf_script.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# forecast_it, ts_it = make_evaluation_predictions(\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bterminator7.cs.rice.edu/home/aw82/StatML/enhancedts-transformers/scripts/EXP-LongForecasting/hf_script.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bterminator7.cs.rice.edu/home/aw82/StatML/enhancedts-transformers/scripts/EXP-LongForecasting/hf_script.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m# evaluator = Evaluator()\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bterminator7.cs.rice.edu/home/aw82/StatML/enhancedts-transformers/scripts/EXP-LongForecasting/hf_script.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m agg_metrics, _ \u001b[39m=\u001b[39m evaluator(\u001b[39miter\u001b[39;49m(d_linear_tss), \u001b[39miter\u001b[39;49m(d_linear_forecasts))\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/gluonts/evaluation/_base.py:264\u001b[0m, in \u001b[0;36mEvaluator.__call__\u001b[0;34m(self, ts_iterator, fcst_iterator, num_series)\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_workers \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m sys\u001b[39m.\u001b[39mplatform \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mwin32\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    261\u001b[0m     mp_pool \u001b[39m=\u001b[39m multiprocessing\u001b[39m.\u001b[39mPool(\n\u001b[1;32m    262\u001b[0m         initializer\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, processes\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_workers\n\u001b[1;32m    263\u001b[0m     )\n\u001b[0;32m--> 264\u001b[0m     rows \u001b[39m=\u001b[39m mp_pool\u001b[39m.\u001b[39;49mmap(\n\u001b[1;32m    265\u001b[0m         func\u001b[39m=\u001b[39;49mpartial(worker_function, \u001b[39mself\u001b[39;49m),\n\u001b[1;32m    266\u001b[0m         iterable\u001b[39m=\u001b[39;49m\u001b[39miter\u001b[39;49m(it),\n\u001b[1;32m    267\u001b[0m         chunksize\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mchunk_size,\n\u001b[1;32m    268\u001b[0m     )\n\u001b[1;32m    269\u001b[0m     mp_pool\u001b[39m.\u001b[39mclose()\n\u001b[1;32m    270\u001b[0m     mp_pool\u001b[39m.\u001b[39mjoin()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/multiprocessing/pool.py:364\u001b[0m, in \u001b[0;36mPool.map\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    359\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmap\u001b[39m(\u001b[39mself\u001b[39m, func, iterable, chunksize\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    360\u001b[0m     \u001b[39m'''\u001b[39;00m\n\u001b[1;32m    361\u001b[0m \u001b[39m    Apply `func` to each element in `iterable`, collecting the results\u001b[39;00m\n\u001b[1;32m    362\u001b[0m \u001b[39m    in a list that is returned.\u001b[39;00m\n\u001b[1;32m    363\u001b[0m \u001b[39m    '''\u001b[39;00m\n\u001b[0;32m--> 364\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_map_async(func, iterable, mapstar, chunksize)\u001b[39m.\u001b[39;49mget()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/multiprocessing/pool.py:765\u001b[0m, in \u001b[0;36mApplyResult.get\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    764\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget\u001b[39m(\u001b[39mself\u001b[39m, timeout\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m--> 765\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwait(timeout)\n\u001b[1;32m    766\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mready():\n\u001b[1;32m    767\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mTimeoutError\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/multiprocessing/pool.py:762\u001b[0m, in \u001b[0;36mApplyResult.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    761\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwait\u001b[39m(\u001b[39mself\u001b[39m, timeout\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m--> 762\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_event\u001b[39m.\u001b[39;49mwait(timeout)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/threading.py:574\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    572\u001b[0m signaled \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flag\n\u001b[1;32m    573\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 574\u001b[0m     signaled \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_cond\u001b[39m.\u001b[39;49mwait(timeout)\n\u001b[1;32m    575\u001b[0m \u001b[39mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/threading.py:312\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[39mtry\u001b[39;00m:    \u001b[39m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    311\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 312\u001b[0m         waiter\u001b[39m.\u001b[39;49macquire()\n\u001b[1;32m    313\u001b[0m         gotit \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    314\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# from gluonts.evaluation import make_evaluation_predictions, Evaluator\n",
    "\n",
    "# forecast_it, ts_it = make_evaluation_predictions(\n",
    "#     dataset=dataset.test,\n",
    "#     predictor=predictor,\n",
    "# )\n",
    "\n",
    "# d_linear_forecasts = list(forecast_it)\n",
    "# d_linear_tss = list(ts_it)\n",
    "\n",
    "# evaluator = Evaluator()\n",
    "\n",
    "agg_metrics, _ = evaluator(iter(d_linear_tss), iter(d_linear_forecasts))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'agg_metrics' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/aw82/StatML/enhancedts-transformers/scripts/EXP-LongForecasting/hf_script.ipynb Cell 20\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bterminator7.cs.rice.edu/home/aw82/StatML/enhancedts-transformers/scripts/EXP-LongForecasting/hf_script.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m dlinear_mase \u001b[39m=\u001b[39m agg_metrics[\u001b[39m\"\u001b[39m\u001b[39mMASE\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bterminator7.cs.rice.edu/home/aw82/StatML/enhancedts-transformers/scripts/EXP-LongForecasting/hf_script.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mDLinear MASE: \u001b[39m\u001b[39m{\u001b[39;00mdlinear_mase\u001b[39m:\u001b[39;00m\u001b[39m.3f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'agg_metrics' is not defined"
     ]
    }
   ],
   "source": [
    "dlinear_mase = agg_metrics[\"MASE\"]\n",
    "print(f\"DLinear MASE: {dlinear_mase:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_gluonts(index):\n",
    "    plt.plot(d_linear_tss[index][-4 * dataset.metadata.prediction_length:].to_timestamp(), label=\"target\")\n",
    "    d_linear_forecasts[index].plot(show_label=True,  color='g')\n",
    "    plt.legend()\n",
    "    plt.gcf().autofmt_xdate()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'd_linear_tss' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/aw82/StatML/enhancedts-transformers/scripts/EXP-LongForecasting/hf_script.ipynb Cell 22\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bterminator7.cs.rice.edu/home/aw82/StatML/enhancedts-transformers/scripts/EXP-LongForecasting/hf_script.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m plot_gluonts(\u001b[39m4\u001b[39;49m)\n",
      "\u001b[1;32m/home/aw82/StatML/enhancedts-transformers/scripts/EXP-LongForecasting/hf_script.ipynb Cell 22\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bterminator7.cs.rice.edu/home/aw82/StatML/enhancedts-transformers/scripts/EXP-LongForecasting/hf_script.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mplot_gluonts\u001b[39m(index):\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bterminator7.cs.rice.edu/home/aw82/StatML/enhancedts-transformers/scripts/EXP-LongForecasting/hf_script.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m     plt\u001b[39m.\u001b[39mplot(d_linear_tss[index][\u001b[39m-\u001b[39m\u001b[39m4\u001b[39m \u001b[39m*\u001b[39m dataset\u001b[39m.\u001b[39mmetadata\u001b[39m.\u001b[39mprediction_length:]\u001b[39m.\u001b[39mto_timestamp(), label\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtarget\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bterminator7.cs.rice.edu/home/aw82/StatML/enhancedts-transformers/scripts/EXP-LongForecasting/hf_script.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m     d_linear_forecasts[index]\u001b[39m.\u001b[39mplot(show_label\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,  color\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mg\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bterminator7.cs.rice.edu/home/aw82/StatML/enhancedts-transformers/scripts/EXP-LongForecasting/hf_script.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m     plt\u001b[39m.\u001b[39mlegend()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'd_linear_tss' is not defined"
     ]
    }
   ],
   "source": [
    "plot_gluonts(4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
